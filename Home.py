import streamlit as st
import pandas as pd
import random
import math
import numpy as np
import unicodedata
import re
import html
import urllib  
import os
from dotenv import load_dotenv

from datetime import datetime

from langchain.docstore.document import Document
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers import BM25Retriever
from langchain.schema import SystemMessage, HumanMessage

# ã‚¿ã‚°å…¥åŠ›UIãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from streamlit_tags import st_tags

load_dotenv()  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
openai.api_key = os.getenv("OPENAI_API_KEY")

# Streamlit ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Booklight AI", page_icon="ğŸ“š", layout="centered")

# å¤–éƒ¨CSSãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ç‰ˆï¼‰
def local_css(file_name):
    with open(file_name) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

local_css("style.css")

# ç”»åƒãƒãƒŠãƒ¼ã‚’è¡¨ç¤º
st.image("images/booklight_ai_banner.png", use_container_width=True)

# -------------------------------------------
# HTMLã‚¿ã‚°é™¤å»ãªã©ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -------------------------------------------
def remove_html_tags(text):
    """HTMLã‚¿ã‚°ã‚’å‰Šé™¤ã™ã‚‹é–¢æ•°"""
    import re
    if not isinstance(text, str):
        text = str(text)
    return re.sub(r'<[^>]*>', '', text)

def sanitize_text_for_display(text):
    """è¡¨ç¤ºç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰HTMLã‚¿ã‚°ã‚’å‰Šé™¤ã—ã€å®‰å…¨ã«ã™ã‚‹"""
    import re
    if not isinstance(text, str):
        text = str(text)
    # ã¾ãšHTMLã‚¿ã‚°ã‚’å‰Šé™¤
    text = re.sub(r'<.*?>', '', text)
    # ã•ã‚‰ã«ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    import html
    text = html.escape(text)
    return text

def get_safe_metadata(doc, key, default=""):
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«å–å¾—ã—ã€HTMLã‚’é™¤å»ã™ã‚‹"""
    value = doc.metadata.get(key, default)
    return sanitize_text_for_display(value)

# -------------------------------------------
# æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
# -------------------------------------------
def normalize_japanese_text(text: str) -> str:
    """
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–
    - å…¨è§’/åŠè§’ã®çµ±ä¸€
    - ä½™åˆ†ãªç©ºç™½é™¤å»
    - å°æ–‡å­—åŒ– ãªã©
    """
    if not isinstance(text, str):
        return ""
    
    # æ­£è¦åŒ–å‡¦ç†ï¼ˆNFKCï¼‰
    text = unicodedata.normalize('NFKC', text)
    
    # å°æ–‡å­—åŒ–
    text = text.lower()
    
    # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# -------------------------------------------
# ãƒã‚¤ãƒ©ã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# -------------------------------------------
@st.cache_resource
def load_highlights():
    """
    docs/KindleHighlights.csv:
      - æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«
      - è‘—è€…
      - ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹
    """
    df = pd.read_csv("docs/KindleHighlights.csv")
    docs = []
    for _, row in df.iterrows():
        # æ­£è¦åŒ–ã‚’é©ç”¨
        normalized_highlight = normalize_japanese_text(row["ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹"])
        doc = Document(
            page_content=normalized_highlight,
            metadata={
                "title": normalize_japanese_text(row["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"]),
                "author": normalize_japanese_text(row["è‘—è€…"]),
                "original_content": row["ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹"],  # è¡¨ç¤ºç”¨ã«å…ƒã®å†…å®¹ã‚‚ä¿æŒ
                "original_title": row["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"],      # å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ä¿æŒ
                "original_author": row["è‘—è€…"]             # å…ƒã®è‘—è€…åã‚‚ä¿æŒ
            }
        )
        docs.append(doc)
    return docs

highlight_docs = load_highlights()

# -------------------------------------------
# æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ« & è¦ç´„ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# -------------------------------------------
@st.cache_resource
def load_book_info():
    """
    docs/BookSummaries.csv:
      - æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«
      - è¦ç´„
    """
    df = pd.read_csv("docs/BookSummaries.csv")
    df["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"].fillna("", inplace=True)
    df["è¦ç´„"].fillna("", inplace=True)
    df = df[df["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"] != ""]
    grouped = df.groupby("æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«")["è¦ç´„"].agg(lambda x: "\n".join(x)).reset_index()

    book_info = {}
    for _, row in grouped.iterrows():
        t = row["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"]
        s = row["è¦ç´„"]
        if not isinstance(s, str):
            s = ""
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‚’æ­£è¦åŒ–
        normalized_title = normalize_japanese_text(t)
        normalized_summary = normalize_japanese_text(s)
        book_info[normalized_title] = {
            "title_text": t,  # å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä¿æŒ
            "summary_text": s,  # å…ƒã®è¦ç´„ã‚’ä¿æŒ
            "normalized_title": normalized_title,
            "normalized_summary": normalized_summary
        }
    return book_info

book_info = load_book_info()

# -------------------------------------------
# OpenAI Embeddings
# -------------------------------------------
embeddings_model = OpenAIEmbeddings(
    model="text-embedding-3-small"  # ä»®ã®ãƒ¢ãƒ‡ãƒ«å(ä¾‹)
)

# BM25ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ï¼‰
bm25_highlight_retriever = BM25Retriever.from_documents(highlight_docs)

# -------------------------------------------
# ãƒã‚¤ãƒ©ã‚¤ãƒˆVectorStore
# -------------------------------------------
@st.cache_resource
def get_highlight_vectorstore():
    return Chroma.from_documents(
        documents=highlight_docs,
        embedding=embeddings_model,
        persist_directory="./csv_chroma_db/highlights_v2"
    )

highlight_vs = get_highlight_vectorstore()

# -------------------------------------------
# æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ï¼†è¦ç´„ã®Embeddingsã‚’ç®¡ç†
# -------------------------------------------
def cosine_sim(vec1, vec2):
    dot = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return float('nan')
    return dot / (norm1 * norm2)

@st.cache_resource
def embed_book_info(book_info_dict):
    res = {}
    for _, data in book_info_dict.items():
        t_text = data["normalized_title"]
        s_text = data["normalized_summary"]
        original_title = data["title_text"]
        if not t_text.strip() and not s_text.strip():
            continue
        title_emb = embeddings_model.embed_query(t_text)
        if s_text.strip():
            summary_emb = embeddings_model.embed_query(s_text)
        else:
            summary_emb = title_emb
        res[original_title] = (title_emb, summary_emb)
    return res

book_embeddings = embed_book_info(book_info)

def rank_books_by_title_and_summary(query: str, alpha=0.5, top_k=5):
    normalized_query = normalize_japanese_text(query)
    query_emb = embeddings_model.embed_query(normalized_query)
    scores = []
    for bk_title, (title_emb, summary_emb) in book_embeddings.items():
        t_score = cosine_sim(query_emb, title_emb)
        s_score = cosine_sim(query_emb, summary_emb)
        final_score = alpha * t_score + (1 - alpha) * s_score
        if math.isnan(final_score):
            final_score = float('-inf')
        scores.append((bk_title, final_score, t_score, s_score))
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[:top_k]

# -------------------------------------------
# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ - ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨BM25ã®çµ„ã¿åˆã‚ã›
# -------------------------------------------
def hybrid_search(query, top_k=20, alpha=0.7):
    """
    Embeddingæ¤œç´¢ã¨BM25æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
    alpha: Embeddingæ¤œç´¢ã®é‡ã¿ï¼ˆ0ï½1ï¼‰
    """
    normalized_query = normalize_japanese_text(query)
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè¡Œ
    vector_results = highlight_vs.similarity_search_with_score(normalized_query, k=top_k)
    
    # BM25æ¤œç´¢ã®å®Ÿè¡Œ
    bm25_results = bm25_highlight_retriever.get_relevant_documents(normalized_query)
    
    # çµæœã®ãƒãƒ¼ã‚¸ã¨é‡ã¿ä»˜ã‘
    merged_results = {}
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã®å‡¦ç†
    for doc, score in vector_results:
        doc_id = doc.page_content
        merged_results[doc_id] = {
            "doc": doc,
            "vector_score": score,
            "bm25_score": 0.0,
            "final_score": alpha * score
        }
    
    # BM25çµæœã®å‡¦ç†
    for i, doc in enumerate(bm25_results[:top_k]):
        doc_id = doc.page_content
        # BM25ã®ãƒ©ãƒ³ã‚¯ã‚’ã‚¹ã‚³ã‚¢ã«å¤‰æ›ï¼ˆç°¡æ˜“çš„ï¼‰
        bm25_score = 1.0 - (i / len(bm25_results[:top_k])) if bm25_results else 0.0
        
        if doc_id in merged_results:
            # æ—¢å­˜çµæœã«åŠ ç®—
            merged_results[doc_id]["bm25_score"] = bm25_score
            merged_results[doc_id]["final_score"] += (1 - alpha) * bm25_score
        else:
            # æ–°è¦è¿½åŠ 
            merged_results[doc_id] = {
                "doc": doc,
                "vector_score": 0.0,
                "bm25_score": bm25_score,
                "final_score": (1 - alpha) * bm25_score
            }
    
    # ã‚½ãƒ¼ãƒˆã—ã¦è¿”å´
    return sorted(
        [item for item in merged_results.values()], 
        key=lambda x: x["final_score"], 
        reverse=True
    )[:top_k]

# -------------------------------------------
# ã‚¯ã‚¨ãƒªæ‹¡å¼µ (LLM ã‚’ç”¨ã„ãŸå¤šæ§˜ãªã‚¯ã‚¨ãƒªç”Ÿæˆ)
# -------------------------------------------
def enhanced_query_expansion(query: str) -> dict:
    """
    ã‚·ãƒãƒ‹ãƒ æ‹¡å¼µã€ã‚¯ã‚¨ãƒªãƒªãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ã®è¿½åŠ 
    """
    if not query.strip():
        return {"original": "", "synonyms": "", "reformulation": ""}
        
    system_msg = SystemMessage(content="You are a helpful assistant for search query enhancement in Japanese.")
    
    # 1. ã‚·ãƒãƒ‹ãƒ æ‹¡å¼µ
    synonym_prompt = f"""
ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹é¡ç¾©èªã¾ãŸã¯é–¢é€£ç”¨èªã‚’5ã¤ã€ã‚³ãƒ³ãƒåŒºåˆ‡ã‚Šã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
è¡¨è¨˜ã‚†ã‚Œã‚„é€ã‚Šä»®åé•ã„ã€ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®é•ã„ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚
ã‚¯ã‚¨ãƒª: "{query}"

ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã€é¡ç¾©èªã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    
    # 2. ã‚¯ã‚¨ãƒªãƒªãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    reformulation_prompt = f"""
ä»¥ä¸‹ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã€åŒã˜æ„å‘³ã‚’æŒã¤åˆ¥ã®è¨€è‘‰ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: "{query}"

ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã€è¨€ã„æ›ãˆãŸè¡¨ç¾ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    
    synonym_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.0)
    
    try:
        synonym_result = synonym_llm([system_msg, HumanMessage(content=synonym_prompt)])
        reformulation_result = synonym_llm([system_msg, HumanMessage(content=reformulation_prompt)])
        
        synonyms = [s.strip() for s in synonym_result.content.split(",")]
        reformulation = reformulation_result.content.strip()
        
        return {
            "original": query,
            "synonyms": query + " " + " ".join(synonyms),
            "reformulation": reformulation
        }
    except Exception as e:
        st.error(f"ã‚¯ã‚¨ãƒªæ‹¡å¼µã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"original": query, "synonyms": query, "reformulation": query}

# -------------------------------------------
# è¤‡æ•°æ¤œç´¢çµæœã®ãƒãƒ¼ã‚¸
# -------------------------------------------
def merge_search_results(result_sets, weights=None):
    """
    è¤‡æ•°ã®æ¤œç´¢çµæœã‚’ãƒãƒ¼ã‚¸ã—ã€é‡ã¿ä»˜ã‘ã—ãŸã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã™ã‚‹
    """
    if weights is None:
        weights = [1.0] * len(result_sets)
    
    if len(result_sets) != len(weights):
        raise ValueError("çµæœã‚»ãƒƒãƒˆã¨é‡ã¿ã®æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“")
    
    merged = {}
    
    for i, results in enumerate(result_sets):
        weight = weights[i]
        for result in results:
            doc = result["doc"]
            doc_id = doc.page_content
            score = result["final_score"] * weight
            
            if doc_id in merged:
                merged[doc_id]["score"] += score
            else:
                merged[doc_id] = {
                    "doc": doc,
                    "score": score,
                    "sources": []
                }
            
            source_info = {
                "type": f"result_set_{i}",
                "score": score
            }
            merged[doc_id]["sources"].append(source_info)
    
    return sorted(merged.values(), key=lambda x: x["score"], reverse=True)

# -------------------------------------------
# å¼•ç”¨UIã®è¡¨ç¤º - ä¿®æ­£ç‰ˆ
# -------------------------------------------
def display_quote(content, title, author):
    """
    å¼•ç”¨ã‚«ãƒ¼ãƒ‰è¡¨ç¤ºUI - ã‚¿ã‚¤ãƒˆãƒ«ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ã—ã€ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ã‚ã‹ã‚Šã‚„ã™ã
    """
    # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    safe_content = html.escape(content)
    safe_title = html.escape(title)
    safe_author = html.escape(author)
    
    # URLç”¨ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    encoded_title = urllib.parse.quote(title)
    detail_link = f"BookDetail?title={encoded_title}"
    
    # å¼•ç”¨ç”¨ã®HTMLã‚’ç”Ÿæˆ (ã‚¿ã‚¤ãƒˆãƒ«/è‘—è€…éƒ¨åˆ†ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ )
    quote_html = f"""
    <div class="random-quote-container">
        <div class="random-quote-mark">"</div>
        <p class="random-quote-text">{safe_content}</p>
        <div class="random-quote-footer">
            <a href="{detail_link}" style="text-decoration: none; color: inherit;">
                {safe_title} / {safe_author}
            </a>
        </div>
    </div>
    """
    
    st.markdown(quote_html, unsafe_allow_html=True)

# æ¤œç´¢çµæœè¡¨ç¤ºç”¨ã®é–¢æ•° - ä¿®æ­£ç‰ˆ
def display_search_results(results, max_chars=300, show_feedback=True):
    """
    æ¤œç´¢çµæœã‚’è¡¨ç¤º
    """
    if not results:
        st.warning("é–¢é€£ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    for i, result in enumerate(results, start=1):
        doc = result["doc"]
        score = result.get("score", None)
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
        title = doc.metadata.get("original_title", doc.metadata.get("title", "ä¸æ˜"))
        author = doc.metadata.get("original_author", doc.metadata.get("author", ""))
        content = doc.metadata.get("original_content", doc.page_content)
        
        # é•·ã•åˆ¶é™
        if len(content) > max_chars:
            display_content = content[:max_chars] + "..."
        else:
            display_content = content
        
        # å¼•ç”¨è¡¨ç¤ºé–¢æ•°ã‚’ä½¿ç”¨
        display_quote(display_content, title, author)

# -------------------------------------------
# ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
# -------------------------------------------
st.markdown("# Random Highlights")
random_count = min(2, len(highlight_docs))
if random_count == 0:
    st.write("ãƒã‚¤ãƒ©ã‚¤ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    random_docs = random.sample(highlight_docs, random_count)
    for i, doc in enumerate(random_docs, start=1):
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
        title = doc.metadata.get("original_title", doc.metadata.get("title", "ä¸æ˜"))
        author = doc.metadata.get("original_author", doc.metadata.get("author", ""))
        content = doc.metadata.get("original_content", doc.page_content)
        
        # é•·ã•åˆ¶é™
        if len(content) > 300:
            display_content = content[:300] + "..."
        else:
            display_content = content
        
        # å¼•ç”¨è¡¨ç¤ºé–¢æ•°ã‚’ä½¿ç”¨
        display_quote(display_content, title, author)

# -------------------------------------------
# ãƒ¢ãƒ¼ãƒ‰é¸æŠ
# -------------------------------------------
st.markdown("# Booklight Chat")
MODE = st.radio("ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã‚“ã§ãã ã•ã„", ["æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", "è­°è«–ãƒ¢ãƒ¼ãƒ‰"], index=0)

# -------------------------------------------
# æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ (æ”¹å–„ç‰ˆ)
# -------------------------------------------
def improved_search_mode(keywords: list, hybrid_alpha=0.7, book_weight=0.3, use_expanded=True):
    if not keywords:
        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    raw_query = " ".join(keywords)
    status = st.empty()
    status.info("æ¤œç´¢æº–å‚™ä¸­...")
    
    if use_expanded:
        expanded = enhanced_query_expansion(raw_query)
        st.markdown(f"**æ¤œç´¢ã‚¯ã‚¨ãƒª**: {raw_query}")
        with st.expander("ã‚¯ã‚¨ãƒªæ‹¡å¼µæƒ…å ±", expanded=False):
            st.markdown(f"**ã‚·ãƒãƒ‹ãƒ æ‹¡å¼µ**: {expanded['synonyms']}")
            st.markdown(f"**è¨€ã„æ›ãˆã‚¯ã‚¨ãƒª**: {expanded['reformulation']}")
    else:
        expanded = {"original": raw_query, "synonyms": raw_query, "reformulation": raw_query}
    
    status.info("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œä¸­...")
    results_original = hybrid_search(raw_query, top_k=10, alpha=hybrid_alpha)
    
    if use_expanded:
        results_synonyms = hybrid_search(expanded['synonyms'], top_k=10, alpha=hybrid_alpha)
        results_reformulation = hybrid_search(expanded['reformulation'], top_k=10, alpha=hybrid_alpha)
        
        status.info("æ¤œç´¢çµæœãƒãƒ¼ã‚¸ä¸­...")
        merged_results = merge_search_results(
            [results_original, results_synonyms, results_reformulation],
            weights=[1.0, 0.8, 0.9]
        )
    else:
        merged_results = results_original
    
    status.info("æ›¸ç±æƒ…å ±ã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸­...")
    book_ranks = rank_books_by_title_and_summary(raw_query, alpha=0.5, top_k=20)
    book_scores = {title: score for title, score, _, _ in book_ranks}
    
    final_results = []
    for result in merged_results:
        doc = result["doc"]
        title = doc.metadata.get("title", "")
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™ï¼ˆç°¡æ˜“çš„ï¼‰
        original_title = ""
        for k, v in book_info.items():
            if normalize_japanese_text(v["title_text"]) == title:
                original_title = v["title_text"]
                break
        
        book_score = book_scores.get(original_title, 0.0)
        final_score = result["score"] + (book_score * book_weight)
        
        final_results.append({
            "doc": doc,
            "score": final_score,
            "original_score": result["score"],
            "book_score": book_score
        })
    
    final_results.sort(key=lambda x: x["score"], reverse=True)
    
    with st.expander("æ¤œç´¢è©³ç´°æƒ…å ±", expanded=False):
        st.write("#### æ›¸ç±ã‚¹ã‚³ã‚¢ (ã‚¿ã‚¤ãƒˆãƒ« & è¦ç´„)")
        for i, (bk_title, final_s, t_s, s_s) in enumerate(book_ranks[:5], start=1):
            disp_final = f"{final_s:.3f}" if final_s != float('-inf') else "nan"
            disp_t = f"{t_s:.3f}" if not math.isnan(t_s) else "nan"
            disp_s = f"{s_s:.3f}" if not math.isnan(s_s) else "nan"
            st.write(f"**[{i}]** {bk_title} : final={disp_final}, title={disp_t}, summary={disp_s}")
    
    status.empty()
    st.subheader("é–¢é€£ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆä¸€è¦§")
    
    if not final_results:
        st.warning("æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
        return
    
    display_search_results(final_results[:15], show_feedback=False)
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = {
        "timestamp": timestamp,
        "query": raw_query,
        "expanded_query": expanded['synonyms'],
        "reformulated_query": expanded['reformulation'],
        "results_count": len(final_results)
    }
    with st.expander("æ¤œç´¢ãƒ­ã‚°", expanded=False):
        st.json(log_entry)

# -------------------------------------------
# è­°è«–ãƒ¢ãƒ¼ãƒ‰
# -------------------------------------------
discussion_prompt_template = """
ã‚ãªãŸã¯æ›¸ç±ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆæƒ…å ±ã‚’å‚ç…§ã§ãã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

1. é–¢é€£ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’å‚è€ƒã«ã—ãªãŒã‚‰ã€è³ªå•ã«å¯¾ã™ã‚‹é‡è¦ãªè¦³ç‚¹ã‚’ç®‡æ¡æ›¸ãã§æ•´ç†ã™ã‚‹ã€‚
2. æ›¸ç±ãƒã‚¤ãƒ©ã‚¤ãƒˆã ã‘ã§ã¯ãªãã€ã‚ãªãŸè‡ªèº«ã®çŸ¥è­˜ã‚‚è¸ã¾ãˆã¦è¿½åŠ ã®è€ƒå¯Ÿã‚’è¿°ã¹ã‚‹ã€‚

æ›¸ç±ãƒã‚¤ãƒ©ã‚¤ãƒˆæƒ…å ±:
{summaries}

ã€è³ªå•ã€‘
{question}

ãã‚Œã§ã¯å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""

DISCUSSION_PROMPT = PromptTemplate(
    template=discussion_prompt_template,
    input_variables=["summaries", "question"]
)

discussion_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4-turbo", temperature=0.1),
    chain_type="stuff",
    retriever=highlight_vs.as_retriever(search_type="similarity", search_kwargs={"k":20}),
    chain_type_kwargs={
        "prompt": DISCUSSION_PROMPT,
        "document_variable_name": "summaries"
    },
    return_source_documents=True
)

def discussion_mode(query: str):
    if not query.strip():
        st.warning("å…¥åŠ›ãŒç©ºã§ã™ã€‚")
        return
    
    status = st.empty()
    status.info("é–¢é€£ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’æ¤œç´¢ä¸­...")
    
    expanded = enhanced_query_expansion(query)
    
    with st.expander("ã‚¯ã‚¨ãƒªæ‹¡å¼µæƒ…å ±", expanded=False):
        st.markdown(f"**åŸæ–‡ã‚¯ã‚¨ãƒª**: {query}")
        st.markdown(f"**ã‚·ãƒãƒ‹ãƒ æ‹¡å¼µ**: {expanded['synonyms']}")
        st.markdown(f"**è¨€ã„æ›ãˆã‚¯ã‚¨ãƒª**: {expanded['reformulation']}")
    
    status.info("å›ç­”ã‚’ç”Ÿæˆä¸­...")
    result = discussion_chain({"query": expanded['synonyms']})
    answer = result["result"]
    source_docs = result["source_documents"]
    
    status.empty()
    st.subheader("è­°è«–ãƒ¢ãƒ¼ãƒ‰å›ç­”")
    st.write(answer)
    
    st.subheader("å‚ç…§ã—ãŸãƒã‚¤ãƒ©ã‚¤ãƒˆ")
    unique_texts = set()
    merged_docs = []
    for d in source_docs:
        txt = d.page_content.strip()
        if txt not in unique_texts:
            unique_texts.add(txt)
            merged_docs.append(d)

    for i, doc in enumerate(merged_docs, start=1):
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
        title = doc.metadata.get("original_title", doc.metadata.get("title", "ä¸æ˜"))
        author = doc.metadata.get("original_author", doc.metadata.get("author", ""))
        content = doc.metadata.get("original_content", doc.page_content)
        
        # é•·ã•åˆ¶é™
        if len(content) > 300:
            display_content = content[:300] + "..."
        else:
            display_content = content
        
        # å¼•ç”¨è¡¨ç¤ºé–¢æ•°ã‚’ä½¿ç”¨
        display_quote(display_content, title, author)

# -------------------------------------------
# UIï¼šæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ or è­°è«–ãƒ¢ãƒ¼ãƒ‰
# -------------------------------------------
if MODE == "æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰":
    st.write("è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢ã§ãã¾ã™ã€‚")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å›ºå®š
    hybrid_alpha = 0.7
    book_weight = 0.3
    use_expanded = True
    
    # ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚°å…¥åŠ›UI
    st.write("### æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    
    if 'search_tags' not in st.session_state:
        st.session_state.search_tags = []
    
    def add_tag():
        if st.session_state.new_tag.strip() and st.session_state.new_tag not in st.session_state.search_tags:
            st.session_state.search_tags.append(st.session_state.new_tag)
            st.session_state.new_tag = ""
    
    def remove_tag(tag_to_remove):
        st.session_state.search_tags = [tag for tag in st.session_state.search_tags if tag != tag_to_remove]
    
    def clear_all_tags():
        st.session_state.search_tags = []
    
    col1, col2 = st.columns([3, 1])
    with col1:
        st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦Enterã§è¿½åŠ ", key="new_tag", on_change=add_tag)
    
    
    if st.session_state.search_tags:
        st.write("##### ç¾åœ¨ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
        tag_cols = st.columns(4)
        for i, tag in enumerate(st.session_state.search_tags):
            with tag_cols[i % 4]:
                st.button(f"Ã— {tag}", key=f"del_{tag}", on_click=remove_tag, args=(tag,))
        
        if st.button("ã™ã¹ã¦ã‚¯ãƒªã‚¢", key="clear_all"):
            clear_all_tags()
    
    if st.button("æ¤œç´¢ã™ã‚‹", key="search_button"):
        if st.session_state.search_tags:
            improved_search_mode(
                st.session_state.search_tags,
                hybrid_alpha=hybrid_alpha,
                book_weight=book_weight,
                use_expanded=use_expanded
            )
        else:
            st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

else:
    user_input = st.text_input("è­°è«–ã—ãŸã„ãƒ†ãƒ¼ãƒã‚„è³ªå•")
    if st.button("è­°è«–ã™ã‚‹"):
        discussion_mode(user_input)
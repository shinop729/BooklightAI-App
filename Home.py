import streamlit as st
import pandas as pd
import random
import os
import openai
import html
import urllib
from dotenv import load_dotenv

from langchain.docstore.document import Document

# ãƒ‡ã‚¶ã‚¤ãƒ³é–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def local_css(file_name):
    with open(file_name) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

# ã‚¢ãƒ—ãƒªå…¨ä½“ã®è¨­å®š
def setup_app():
    # ç’°å¢ƒå¤‰æ•°ã®ãƒ­ãƒ¼ãƒ‰
    load_dotenv()
    openai.api_key = os.getenv("OPENAI_API_KEY")
    
    # ãƒšãƒ¼ã‚¸è¨­å®š
    st.set_page_config(
        page_title="Booklight AI", 
        page_icon="ğŸ“š", 
        layout="centered",
        initial_sidebar_state="expanded"
    )
    
    # CSSã®ãƒ­ãƒ¼ãƒ‰
    local_css("style.css")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.image("images/booklight_ai_banner.png", use_column_width=True)
    st.sidebar.title("Booklight AI")
    st.sidebar.markdown("ğŸ“š ã‚ãªãŸã®èª­æ›¸ã‚’AIãŒç…§ã‚‰ã™")
    
    # åŒºåˆ‡ã‚Šç·š
    st.sidebar.markdown("---")

# å¼•ç”¨UIã®è¡¨ç¤º
def display_quote(content, title, author):
    """
    å¼•ç”¨ã‚«ãƒ¼ãƒ‰è¡¨ç¤ºUI - ã‚¿ã‚¤ãƒˆãƒ«ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ã—ã€ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ã‚ã‹ã‚Šã‚„ã™ã
    """
    # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    safe_content = html.escape(content)
    safe_title = html.escape(title)
    safe_author = html.escape(author)
    
    # URLç”¨ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    encoded_title = urllib.parse.quote(title)
    detail_link = f"BookDetail?title={encoded_title}"
    
    # å¼•ç”¨ç”¨ã®HTMLã‚’ç”Ÿæˆ
    quote_html = f"""
    <div class="random-quote-container">
        <div class="random-quote-mark">"</div>
        <p class="random-quote-text">{safe_content}</p>
        <div class="random-quote-footer">
            <a href="{detail_link}" style="text-decoration: none; color: inherit;">
                {safe_title} / {safe_author}
            </a>
        </div>
    </div>
    """
    
    st.markdown(quote_html, unsafe_allow_html=True)

# æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
def normalize_japanese_text(text: str) -> str:
    """
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–
    - å…¨è§’/åŠè§’ã®çµ±ä¸€
    - ä½™åˆ†ãªç©ºç™½é™¤å»
    - å°æ–‡å­—åŒ– ãªã©
    """
    if not isinstance(text, str):
        return ""
    
    # æ­£è¦åŒ–å‡¦ç†ï¼ˆNFKCï¼‰
    text = unicodedata.normalize('NFKC', text)
    
    # å°æ–‡å­—åŒ–
    text = text.lower()
    
    # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# ãƒã‚¤ãƒ©ã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆä»–ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚‚å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«å…¬é–‹ï¼‰
@st.cache_resource
def load_highlights():
    """
    docs/KindleHighlights.csv:
      - æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«
      - è‘—è€…
      - ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹
    """
    df = pd.read_csv("docs/KindleHighlights.csv")
    docs = []
    for _, row in df.iterrows():
        # æ­£è¦åŒ–ã‚’é©ç”¨
        normalized_highlight = normalize_japanese_text(row["ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹"])
        doc = Document(
            page_content=normalized_highlight,
            metadata={
                "title": normalize_japanese_text(row["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"]),
                "author": normalize_japanese_text(row["è‘—è€…"]),
                "original_content": row["ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹"],  # è¡¨ç¤ºç”¨ã«å…ƒã®å†…å®¹ã‚‚ä¿æŒ
                "original_title": row["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"],      # å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ä¿æŒ
                "original_author": row["è‘—è€…"]             # å…ƒã®è‘—è€…åã‚‚ä¿æŒ
            }
        )
        docs.append(doc)
    return docs

# æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ« & è¦ç´„ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
@st.cache_resource
def load_book_info():
    """
    docs/BookSummaries.csv:
      - æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«
      - è¦ç´„
    """
    df = pd.read_csv("docs/BookSummaries.csv")
    df["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"].fillna("", inplace=True)
    df["è¦ç´„"].fillna("", inplace=True)
    df = df[df["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"] != ""]
    grouped = df.groupby("æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«")["è¦ç´„"].agg(lambda x: "\n".join(x)).reset_index()

    book_info = {}
    for _, row in grouped.iterrows():
        t = row["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"]
        s = row["è¦ç´„"]
        if not isinstance(s, str):
            s = ""
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‚’æ­£è¦åŒ–
        normalized_title = normalize_japanese_text(t)
        normalized_summary = normalize_japanese_text(s)
        book_info[normalized_title] = {
            "title_text": t,  # å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä¿æŒ
            "summary_text": s,  # å…ƒã®è¦ç´„ã‚’ä¿æŒ
            "normalized_title": normalized_title,
            "normalized_summary": normalized_summary
        }
    return book_info

# ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®è¡¨ç¤º
def main():
    # ã‚¢ãƒ—ãƒªå…¨ä½“ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    setup_app()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
    st.sidebar.markdown("### ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
    pages = {
        "ğŸ  ãƒ›ãƒ¼ãƒ ": "/",
        "ğŸ” æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰": "Search",
        "ğŸ’¬ ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰": "Chat",
        "ğŸ“š æ›¸ç±ä¸€è¦§": "BookList"
    }

    for page_name, page_url in pages.items():
        st.sidebar.page_link(page_url, label=page_name)
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    st.image("images/booklight_ai_banner.png", use_container_width=True)
    st.title("Booklight AI ã¸ã‚ˆã†ã“ãï¼")
    st.markdown("""
    Booklight AIã¯ã‚ãªãŸã®èª­æ›¸ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’ç®¡ç†ã—ã€çŸ¥è­˜ã®æ¢ç´¢ã‚’ãŠæ‰‹ä¼ã„ã—ã¾ã™ã€‚
    
    **ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ä»¥ä¸‹ã®æ©Ÿèƒ½ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ï¼š**
    - **ğŸ” æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰**: ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
    - **ğŸ’¬ ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰**: ãƒã‚¤ãƒ©ã‚¤ãƒˆã«åŸºã¥ã„ãŸä¼šè©±
    - **ğŸ“š æ›¸ç±ä¸€è¦§**: ç™»éŒ²æ¸ˆã¿ã®æ›¸ç±ã‚’é–²è¦§
    """)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
    st.markdown("## ä»Šæ—¥ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ãƒ©ã‚¤ãƒˆ")
    highlight_docs = load_highlights()
    random_count = min(2, len(highlight_docs))
    if random_count == 0:
        st.write("ãƒã‚¤ãƒ©ã‚¤ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        random_docs = random.sample(highlight_docs, random_count)
        for doc in random_docs:
            # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
            title = doc.metadata.get("original_title", doc.metadata.get("title", "ä¸æ˜"))
            author = doc.metadata.get("original_author", doc.metadata.get("author", ""))
            content = doc.metadata.get("original_content", doc.page_content)
            
            # é•·ã•åˆ¶é™
            if len(content) > 300:
                display_content = content[:300] + "..."
            else:
                display_content = content
            
            # å¼•ç”¨è¡¨ç¤ºé–¢æ•°ã‚’ä½¿ç”¨
            display_quote(display_content, title, author)

if __name__ == "__main__":
    main()
import streamlit as st
import pandas as pd
import os
import urllib.parse
import unicodedata
import re
import random
from langchain.schema import Document
from dotenv import load_dotenv
import openai
import urllib.parse

def local_css(file_name):
    """Load and inject a local CSS file into the Streamlit app"""
    with open(file_name) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

# ã‚¢ãƒ—ãƒªå…¨ä½“ã®è¨­å®š
def setup_app():
    load_dotenv()
    openai.api_key = os.getenv("OPENAI_API_KEY")
    
    st.set_page_config(page_title="Booklight AI", layout="wide")
    st.sidebar.image("images/booklight_ai_banner.png", use_container_width=True)
    st.sidebar.title("Booklight AI")
    st.sidebar.markdown("ğŸ“š ã‚ãªãŸã®èª­æ›¸ã‚’AIãŒç…§ã‚‰ã™")
    st.sidebar.markdown("---")

import urllib.parse
import unicodedata
import re
import pandas as pd
import random
import os
import openai
from langchain_core.documents import Document

def normalize_japanese_text(text: str) -> str:
    import unicodedata
    import re
    text = unicodedata.normalize('NFKC', text)
    text = text.lower()
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def load_highlights():
    df = pd.read_csv("docs/KindleHighlights.csv")
    docs = []
    for _, row in df.iterrows():
        normalized_highlight = normalize_japanese_text(row["ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹"])
        doc = Document(
            page_content=normalized_highlight,
            metadata={
                "original_title": row["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"],
                "original_author": row["è‘—è€…"]
            }
        )
        docs.append(doc)
    return docs

@st.cache_resource
def load_book_info():
    df = pd.read_csv("docs/KindleHighlights.csv")
    df = df[df["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"] != ""]
    
    # Group by book title and aggregate highlights
    grouped = df.groupby("æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«")["ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹"].agg(lambda x: "\n".join(x)).reset_index()
    
    book_info = {}
    for _, row in grouped.iterrows():
        title = row["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"]
        # Get author for this book (taking the first one if multiple)
        author = df[df["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"] == title]["è‘—è€…"].iloc[0] if not df[df["æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«"] == title]["è‘—è€…"].empty else ""
        
        # Create a dictionary with the required structure for Search.py
        book_info[title] = {
            "title_text": title,
            "normalized_title": normalize_japanese_text(title),
            "normalized_summary": normalize_japanese_text(row["ãƒã‚¤ãƒ©ã‚¤ãƒˆå†…å®¹"]),
            "author": author
        }
    return book_info

def display_quote(content, title, author, index=0):
    """
    Home.pyç”¨ã®å¼•ç”¨è¡¨ç¤ºé–¢æ•° - æ›¸ç±è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ä»˜ã
    """
    # ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ç”¨ã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
    quote_html = f"""
    <div style="padding:15px; border-radius:8px; background-color:#2a2a2a; margin-bottom:15px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
        <p style="color:#ffffff; font-size:16px; line-height:1.6; margin-bottom:12px;">{content}</p>
        <div style="text-align:right;">
            <span style="color:#4da6ff; font-weight:500;">{title} / {author}</span>
        </div>
    </div>
    """
    st.markdown(quote_html, unsafe_allow_html=True)
    
    # è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
    col1, col2 = st.columns([3, 1])
    with col2:
        if st.button(f"è©³ç´°ã‚’è¦‹ã‚‹", key=f"home_detail_{index}"):
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä¿å­˜
            st.session_state.selected_book_title = title
            # BookDetailãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
            st.switch_page("pages/BookDetail.py")

def main():
    setup_app()
    highlight_docs = load_highlights()

    st.sidebar.markdown("[ğŸ” æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰](pages/Search.py)")
    st.sidebar.markdown("[ğŸ’¬ ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰](pages/Chat.py)")
    st.sidebar.markdown("[ğŸ“š æ›¸ç±ä¸€è¦§](pages/BookList.py)")

    if not highlight_docs:
        st.write("ãƒã‚¤ãƒ©ã‚¤ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        random_docs = random.sample(highlight_docs, min(3, len(highlight_docs)))
        for i, doc in enumerate(random_docs):
            title = doc.metadata.get("original_title", "ä¸æ˜ãªã‚¿ã‚¤ãƒˆãƒ«")
            author = doc.metadata.get("original_author", "ä¸æ˜ãªè‘—è€…")
            content = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
            display_quote(content, title, author, i)

if __name__ == "__main__":
    main()

import streamlit as st
import pandas as pd
import math
import numpy as np
import unicodedata
import re
import os
import sys
import openai
import html
import urllib
from dotenv import load_dotenv
from datetime import datetime
from pathlib import Path

from langchain.docstore.document import Document
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.retrievers import BM25Retriever
from langchain.schema import SystemMessage, HumanMessage

# ã‚¿ã‚°å…¥åŠ›UIãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from streamlit_tags import st_tags

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆHomeãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ï¼‰
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import auth
from progress_display import display_summary_progress_in_sidebar

# Home.pyã‹ã‚‰å…±é€šé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from Home import display_quote, load_highlights, local_css, normalize_japanese_text, load_book_info, load_user_highlights

# ç’°å¢ƒå¤‰æ•°ã®ãƒ­ãƒ¼ãƒ‰
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ | Booklight AI", 
    page_icon="ğŸ”",
    layout="centered",
    initial_sidebar_state="expanded"
)

# CSSã®ãƒ­ãƒ¼ãƒ‰
local_css("style.css")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
st.sidebar.image("images/booklight_ai_banner.png", use_container_width=True)
st.sidebar.title("Booklight AI")
st.sidebar.markdown("ğŸ“š ã‚ãªãŸã®èª­æ›¸ã‚’AIãŒç…§ã‚‰ã™")
st.sidebar.markdown("---")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ­ã‚°ã‚¤ãƒ³/ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
if auth.is_user_authenticated():
    user_info = st.session_state.user_info
    st.sidebar.markdown(f"### ã‚ˆã†ã“ãã€{user_info.get('name', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼')}ã•ã‚“ï¼")
    st.sidebar.markdown(f"ğŸ“§ {user_info.get('email', '')}")
    
    if st.sidebar.button("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"):
        auth.logout()
        st.rerun()  # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰
else:
    st.sidebar.markdown("### ãƒ­ã‚°ã‚¤ãƒ³")
    auth_url = auth.get_google_auth_url()
    if auth_url:
        st.sidebar.markdown(f"[Googleã§ãƒ­ã‚°ã‚¤ãƒ³]({auth_url})")
    else:
        st.sidebar.error("èªè¨¼è¨­å®šãŒä¸å®Œå…¨ã§ã™ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
st.sidebar.markdown("---")
st.sidebar.markdown("### ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
st.sidebar.markdown("[ğŸ  ãƒ›ãƒ¼ãƒ ](Home.py)")
st.sidebar.markdown("[ğŸ” æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰](pages/Search.py)")
st.sidebar.markdown("[ğŸ’¬ ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰](pages/Chat.py)")
st.sidebar.markdown("[ğŸ“š æ›¸ç±ä¸€è¦§](pages/BookList.py)")
st.sidebar.markdown("[ğŸ“¤ ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰](pages/Upload.py)")

# ã‚µãƒãƒªç”Ÿæˆã®é€²æ—çŠ¶æ³ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤º
display_summary_progress_in_sidebar()

# èªè¨¼ãƒ•ãƒ­ãƒ¼ã®å‡¦ç†
auth_success = auth.handle_auth_flow()
if auth_success:
    st.success("ãƒ­ã‚°ã‚¤ãƒ³ã«æˆåŠŸã—ã¾ã—ãŸï¼")
    st.rerun()  # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰

# -------------------------------------------
# OpenAI Embeddings
# -------------------------------------------
embeddings_model = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
if auth.is_user_authenticated():
    user_id = auth.get_current_user_id()
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®æ›¸ç±æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    book_info = load_book_info(user_id)
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’èª­ã¿è¾¼ã¿
    highlight_docs = load_user_highlights(user_id)
    st.info(f"{st.session_state.user_info.get('name', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼')}ã•ã‚“ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢ã—ã¾ã™ã€‚")
else:
    # å…±é€šã®æ›¸ç±æƒ…å ±ã¨ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’èª­ã¿è¾¼ã¿
    book_info = load_book_info()
    highlight_docs = load_highlights()

# BM25ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ï¼‰
bm25_highlight_retriever = BM25Retriever.from_documents(highlight_docs)

# -------------------------------------------
# ãƒã‚¤ãƒ©ã‚¤ãƒˆVectorStore
# -------------------------------------------
@st.cache_resource
def get_highlight_vectorstore(_docs):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
    if auth.is_user_authenticated():
        user_id = auth.get_current_user_id()
        persist_dir = f"./csv_chroma_db/highlights_user_{user_id}"
    else:
        persist_dir = "./csv_chroma_db/highlights_v2"
    
    return Chroma.from_documents(
        documents=_docs,
        embedding=embeddings_model,
        persist_directory=persist_dir
    )

highlight_vs = get_highlight_vectorstore(highlight_docs)

# -------------------------------------------
# æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ï¼†è¦ç´„ã®Embeddingsã‚’ç®¡ç†
# -------------------------------------------
def cosine_sim(vec1, vec2):
    dot = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return float('nan')
    return dot / (norm1 * norm2)

@st.cache_resource
def embed_book_info(book_info_dict):
    res = {}
    for _, data in book_info_dict.items():
        t_text = data["normalized_title"]
        s_text = data["normalized_summary"]
        original_title = data["title_text"]
        if not t_text.strip() and not s_text.strip():
            continue
        title_emb = embeddings_model.embed_query(t_text)
        if s_text.strip():
            summary_emb = embeddings_model.embed_query(s_text)
        else:
            summary_emb = title_emb
        res[original_title] = (title_emb, summary_emb)
    return res

book_embeddings = embed_book_info(book_info)

def rank_books_by_title_and_summary(query: str, alpha=0.5, top_k=5):
    normalized_query = normalize_japanese_text(query)
    query_emb = embeddings_model.embed_query(normalized_query)
    scores = []
    for bk_title, (title_emb, summary_emb) in book_embeddings.items():
        t_score = cosine_sim(query_emb, title_emb)
        s_score = cosine_sim(query_emb, summary_emb)
        final_score = alpha * t_score + (1 - alpha) * s_score
        if math.isnan(final_score):
            final_score = float('-inf')
        scores.append((bk_title, final_score, t_score, s_score))
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[:top_k]

# -------------------------------------------
# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ - ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨BM25ã®çµ„ã¿åˆã‚ã›
# -------------------------------------------
def hybrid_search(query, top_k=20, alpha=0.7):
    """
    Embeddingæ¤œç´¢ã¨BM25æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
    alpha: Embeddingæ¤œç´¢ã®é‡ã¿ï¼ˆ0ï½1ï¼‰
    """
    normalized_query = normalize_japanese_text(query)
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè¡Œ
    vector_results = highlight_vs.similarity_search_with_score(normalized_query, k=top_k)
    
    # BM25æ¤œç´¢ã®å®Ÿè¡Œ
    bm25_results = bm25_highlight_retriever.get_relevant_documents(normalized_query)
    
    # çµæœã®ãƒãƒ¼ã‚¸ã¨é‡ã¿ä»˜ã‘
    merged_results = {}
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã®å‡¦ç†
    for doc, score in vector_results:
        doc_id = doc.page_content
        merged_results[doc_id] = {
            "doc": doc,
            "vector_score": score,
            "bm25_score": 0.0,
            "final_score": alpha * score
        }
    
    # BM25çµæœã®å‡¦ç†
    for i, doc in enumerate(bm25_results[:top_k]):
        doc_id = doc.page_content
        # BM25ã®ãƒ©ãƒ³ã‚¯ã‚’ã‚¹ã‚³ã‚¢ã«å¤‰æ›ï¼ˆç°¡æ˜“çš„ï¼‰
        bm25_score = 1.0 - (i / len(bm25_results[:top_k])) if bm25_results else 0.0
        
        if doc_id in merged_results:
            # æ—¢å­˜çµæœã«åŠ ç®—
            merged_results[doc_id]["bm25_score"] = bm25_score
            merged_results[doc_id]["final_score"] += (1 - alpha) * bm25_score
        else:
            # æ–°è¦è¿½åŠ 
            merged_results[doc_id] = {
                "doc": doc,
                "vector_score": 0.0,
                "bm25_score": bm25_score,
                "final_score": (1 - alpha) * bm25_score
            }
    
    # ã‚½ãƒ¼ãƒˆã—ã¦è¿”å´
    return sorted(
        [item for item in merged_results.values()], 
        key=lambda x: x["final_score"], 
        reverse=True
    )[:top_k]

# -------------------------------------------
# ã‚¯ã‚¨ãƒªæ‹¡å¼µ (LLM ã‚’ç”¨ã„ãŸå¤šæ§˜ãªã‚¯ã‚¨ãƒªç”Ÿæˆ)
# -------------------------------------------
def enhanced_query_expansion(query: str) -> dict:
    """
    ã‚·ãƒãƒ‹ãƒ æ‹¡å¼µã€ã‚¯ã‚¨ãƒªãƒªãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ã®è¿½åŠ 
    """
    if not query.strip():
        return {"original": "", "synonyms": "", "reformulation": ""}
        
    system_msg = SystemMessage(content="You are a helpful assistant for search query enhancement in Japanese.")
    
    # 1. ã‚·ãƒãƒ‹ãƒ æ‹¡å¼µ
    synonym_prompt = f"""
ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹é¡ç¾©èªã¾ãŸã¯é–¢é€£ç”¨èªã‚’5ã¤ã€ã‚³ãƒ³ãƒåŒºåˆ‡ã‚Šã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
è¡¨è¨˜ã‚†ã‚Œã‚„é€ã‚Šä»®åé•ã„ã€ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®é•ã„ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚
ã‚¯ã‚¨ãƒª: "{query}"

ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã€é¡ç¾©èªã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    
    # 2. ã‚¯ã‚¨ãƒªãƒªãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    reformulation_prompt = f"""
ä»¥ä¸‹ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã€åŒã˜æ„å‘³ã‚’æŒã¤åˆ¥ã®è¨€è‘‰ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: "{query}"

ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã€è¨€ã„æ›ãˆãŸè¡¨ç¾ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    
    synonym_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.0)
    
    try:
        synonym_result = synonym_llm([system_msg, HumanMessage(content=synonym_prompt)])
        reformulation_result = synonym_llm([system_msg, HumanMessage(content=reformulation_prompt)])
        
        synonyms = [s.strip() for s in synonym_result.content.split(",")]
        reformulation = reformulation_result.content.strip()
        
        return {
            "original": query,
            "synonyms": query + " " + " ".join(synonyms),
            "reformulation": reformulation
        }
    except Exception as e:
        st.error(f"ã‚¯ã‚¨ãƒªæ‹¡å¼µã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"original": query, "synonyms": query, "reformulation": query}

# -------------------------------------------
# è¤‡æ•°æ¤œç´¢çµæœã®ãƒãƒ¼ã‚¸
# -------------------------------------------
def merge_search_results(result_sets, weights=None):
    """
    è¤‡æ•°ã®æ¤œç´¢çµæœã‚’ãƒãƒ¼ã‚¸ã—ã€é‡ã¿ä»˜ã‘ã—ãŸã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã™ã‚‹
    """
    if weights is None:
        weights = [1.0] * len(result_sets)
    
    if len(result_sets) != len(weights):
        raise ValueError("çµæœã‚»ãƒƒãƒˆã¨é‡ã¿ã®æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“")
    
    merged = {}
    
    for i, results in enumerate(result_sets):
        weight = weights[i]
        for result in results:
            doc = result["doc"]
            doc_id = doc.page_content
            score = result["final_score"] * weight
            
            if doc_id in merged:
                merged[doc_id]["score"] += score
            else:
                merged[doc_id] = {
                    "doc": doc,
                    "score": score,
                    "sources": []
                }
            
            source_info = {
                "type": f"result_set_{i}",
                "score": score
            }
            merged[doc_id]["sources"].append(source_info)
    
    return sorted(merged.values(), key=lambda x: x["score"], reverse=True)

# æ¤œç´¢çµæœè¡¨ç¤ºç”¨ã®é–¢æ•°
def display_search_results(results, max_chars=300, show_feedback=True):
    """
    æ¤œç´¢çµæœã‚’è¡¨ç¤º
    """
    if not results:
        st.warning("é–¢é€£ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    for i, result in enumerate(results, start=1):
        doc = result["doc"]
        score = result.get("score", None)
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
        title = doc.metadata.get("original_title", doc.metadata.get("title", "ä¸æ˜"))
        author = doc.metadata.get("original_author", doc.metadata.get("author", ""))
        content = doc.metadata.get("original_content", doc.page_content)
        
        # é•·ã•åˆ¶é™
        if len(content) > max_chars:
            display_content = content[:max_chars] + "..."
        else:
            display_content = content
        
        # å¼•ç”¨è¡¨ç¤ºé–¢æ•°ã‚’ä½¿ç”¨
        display_quote(display_content, title, author)

# -------------------------------------------
# æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ (æ”¹å–„ç‰ˆ)
# -------------------------------------------
def improved_search_mode(keywords: list, hybrid_alpha=0.7, book_weight=0.3, use_expanded=True):
    if not keywords:
        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    raw_query = " ".join(keywords)
    status = st.empty()
    status.info("æ¤œç´¢æº–å‚™ä¸­...")
    
    if use_expanded:
        expanded = enhanced_query_expansion(raw_query)
        st.markdown(f"**æ¤œç´¢ã‚¯ã‚¨ãƒª**: {raw_query}")
        with st.expander("ã‚¯ã‚¨ãƒªæ‹¡å¼µæƒ…å ±", expanded=False):
            st.markdown(f"**ã‚·ãƒãƒ‹ãƒ æ‹¡å¼µ**: {expanded['synonyms']}")
            st.markdown(f"**è¨€ã„æ›ãˆã‚¯ã‚¨ãƒª**: {expanded['reformulation']}")
    else:
        expanded = {"original": raw_query, "synonyms": raw_query, "reformulation": raw_query}
    
    status.info("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œä¸­...")
    results_original = hybrid_search(raw_query, top_k=10, alpha=hybrid_alpha)
    
    if use_expanded:
        results_synonyms = hybrid_search(expanded['synonyms'], top_k=10, alpha=hybrid_alpha)
        results_reformulation = hybrid_search(expanded['reformulation'], top_k=10, alpha=hybrid_alpha)
        
        status.info("æ¤œç´¢çµæœãƒãƒ¼ã‚¸ä¸­...")
        merged_results = merge_search_results(
            [results_original, results_synonyms, results_reformulation],
            weights=[1.0, 0.8, 0.9]
        )
    else:
        merged_results = results_original
    
    status.info("æ›¸ç±æƒ…å ±ã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸­...")
    book_ranks = rank_books_by_title_and_summary(raw_query, alpha=0.5, top_k=20)
    book_scores = {title: score for title, score, _, _ in book_ranks}
    
    final_results = []
    for result in merged_results:
        doc = result["doc"]
        title = doc.metadata.get("title", "")
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™ï¼ˆç°¡æ˜“çš„ï¼‰
        original_title = ""
        for k, v in book_info.items():
            if normalize_japanese_text(v["title_text"]) == title:
                original_title = v["title_text"]
                break
        
        book_score = book_scores.get(original_title, 0.0)
        final_score = result["score"] + (book_score * book_weight)
        
        final_results.append({
            "doc": doc,
            "score": final_score,
            "original_score": result["score"],
            "book_score": book_score
        })
    
    final_results.sort(key=lambda x: x["score"], reverse=True)
    
    with st.expander("æ¤œç´¢è©³ç´°æƒ…å ±", expanded=False):
        st.write("#### æ›¸ç±ã‚¹ã‚³ã‚¢ (ã‚¿ã‚¤ãƒˆãƒ« & è¦ç´„)")
        for i, (bk_title, final_s, t_s, s_s) in enumerate(book_ranks[:5], start=1):
            disp_final = f"{final_s:.3f}" if final_s != float('-inf') else "nan"
            disp_t = f"{t_s:.3f}" if not math.isnan(t_s) else "nan"
            disp_s = f"{s_s:.3f}" if not math.isnan(s_s) else "nan"
            st.write(f"**[{i}]** {bk_title} : final={disp_final}, title={disp_t}, summary={disp_s}")
    
    status.empty()
    st.subheader("é–¢é€£ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆä¸€è¦§")
    
    if not final_results:
        st.warning("æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
        return
    
    display_search_results(final_results[:15], show_feedback=False)
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = {
        "timestamp": timestamp,
        "query": raw_query,
        "expanded_query": expanded['synonyms'],
        "reformulated_query": expanded['reformulation'],
        "results_count": len(final_results)
    }
    with st.expander("æ¤œç´¢ãƒ­ã‚°", expanded=False):
        st.json(log_entry)

# ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
st.title("ğŸ” æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰")
st.write("è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢ã§ãã¾ã™ã€‚")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å›ºå®š
hybrid_alpha = 0.7
book_weight = 0.3
use_expanded = True

# ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚°å…¥åŠ›UI
st.write("### æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

if 'search_tags' not in st.session_state:
    st.session_state.search_tags = []

# ã‚¿ã‚°è¿½åŠ ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
def add_tag():
    if st.session_state.new_tag.strip() and st.session_state.new_tag not in st.session_state.search_tags:
        st.session_state.search_tags.append(st.session_state.new_tag)
        # æ¬¡å›ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ç”¨ã«ã‚¯ãƒªã‚¢ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
        st.session_state.clear_tag_input = True
        st.rerun()

def remove_tag(tag_to_remove):
    st.session_state.search_tags = [tag for tag in st.session_state.search_tags if tag != tag_to_remove]
    st.rerun()

def clear_all_tags():
    st.session_state.search_tags = []
    st.rerun()

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã®åˆæœŸåŒ–
if "new_tag" not in st.session_state:
    st.session_state.new_tag = ""

# ã‚¯ãƒªã‚¢ãƒ•ãƒ©ã‚°ã®å‡¦ç†
if "clear_tag_input" in st.session_state and st.session_state.clear_tag_input:
    st.session_state.new_tag = ""
    st.session_state.clear_tag_input = False

col1, col2 = st.columns([3, 1])
with col1:
    st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦Enterã§è¿½åŠ ", key="new_tag", on_change=add_tag)


if st.session_state.search_tags:
    st.write("##### ç¾åœ¨ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
    tag_cols = st.columns(4)
    for i, tag in enumerate(st.session_state.search_tags):
        with tag_cols[i % 4]:
            st.button(f"Ã— {tag}", key=f"del_{tag}", on_click=remove_tag, args=(tag,))
    
    if st.button("ã™ã¹ã¦ã‚¯ãƒªã‚¢", key="clear_all"):
        clear_all_tags()

if st.button("æ¤œç´¢ã™ã‚‹", key="search_button"):
    if st.session_state.search_tags:
        improved_search_mode(
            st.session_state.search_tags,
            hybrid_alpha=hybrid_alpha,
            book_weight=book_weight,
            use_expanded=use_expanded
        )
    else:
        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
